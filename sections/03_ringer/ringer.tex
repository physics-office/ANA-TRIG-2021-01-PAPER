\section{The \rnn{} Algorithm}%
\label{sec:neuralringer}

This section describes the \rnn{} concepts and the resulting algorithm used for Run~2
operation. Considerations about data sets (Section~\ref{ssec:dataset}) and the \TnP method
(Section~\ref{ssec:tnp}) employed to extract signal and background samples
from real ATLAS data start this section. Section~\ref{ssec:rnn_for_online_and_eletrons} refers to the
decisions made for the \rnn{} development in order to optimize its contributions in saving processing resources. Aiming at better exploring the discriminative
power of ring sums for electron triggers (Section~\ref{ssec:ringer_id}),
an ensemble of neural networks is employed. Its training procedure and how trigger decisions are made are covered in Section~\ref{sec:tuning}.




\subsection{Data sets and Event Selection}%
\label{ssec:dataset}

The training procedure for 2017 operation was built with samples of 2015 simulated $Z\rightarrow ee$ decay selected by the $Z\rightarrow ee$ \TnP method (see Section~\ref{ssec:tnp}). Background samples for electron processes were simulated. A filter was applied to enrich the sample in electron backgrounds selecting events which have particles produced in the hard scatter with a summed transverse energy exceeding 17 GeV in a region of $\Delta\eta\times\Delta\phi=0.1\times0.1$. During 2016 data taking, the pile-up reached was 40 interactions per bunch crossing.
Here, both simulated data include a pile-up of 60 interactions in average, per bunch crossing, which had never been seen before in collision data until the end of 2017, when the observed peek pile-up exceeded 60. Recently, in 2023, the highest level of pile-up observed exceeded 60 again.

After the training stage, thresholds on the discriminating score must be determined for selection of good electron candidates. To set the correct value without causing any inefficiency in the end of the HLT in data, the full \textit{pp} collision data set recorded by ATLAS in 2016 with LHC operating at a centre-of-mass energy of $\sqrt{s}=13\text{ TeV}$ during of Run 2 was used. The electrons were selected by $Z\rightarrow ee$ \TnP method and the offline tight criteria. For background, objects rejected by the $Z\rightarrow ee$ \TnP method, that does not belong to any tag-probe electron pair, and reproved by the loosest offline available criteria were used. For the 2018 operation, the purely data-driven strategy was used, selecting events from the 2017 recorded data to train the models and adjust all thresholds.

The tuning and analysis performed on data recorded have their quality ensured by the data quality procedure. 

%\input{sections/context/tables/event_selection}


\subsubsection{\TnP Method}\label{ssec:tnp}

The \tnp{} method~\cite{PERF-2016-01} selects, from known resonances such as \Zee{}, an unbiased sample of `probe' electrons by using strict selection requirements on the second `tag' object. For electrons coming from $Z$ boson decay, the selection of two EM objects with an invariant mass around the $Z$ mass with opposite charge is enough to get a pure sample of electrons as soon as one of those objects is tagged as a well-identified electron. The remaining object can then be assumed to be an electron with a very high underlying purity. This object can then be used as a probe to test the electron trigger efficiency. The selection of the tag particle is done according to offline analysis criteria to make sure that the trigger selection introduces a minimal loss of those events at the final physics analysis level~\cite{aaboud2019electron}. A single electron is used as a tag to select the event at the trigger level, allowing the second electron to be used as a probe to measure trigger algorithm performance~\cite{aad2020performance}.


\subsection{Algorithm Development}\label{ssec:rnn_for_online_and_eletrons}

The \rnn{} algorithm was initially proposed for electron-jet discrimination. 
During Run 2, both online and offline versions
were considered for  development, but the scarcity of available human resources restricted the scope of developments, which focused on reducing CPU demands of the online electron triggers. Within the electron triggers, \hlt{} selection is
specially relevant to single object triggers, which, in turn, use the most of the trigger bandwidth~\cite{aad2020performance}. Particularly, the primary single electron triggers
are currently evaluated and developed using \Zee{} \tnp{} samples 
(Section~\ref{ssec:tnp}), in which most electrons have $\et>\SI{15}{\GeV}$, which is the region of interest in physics studies and on which the \rnn{} developments focus. The result of this effort
follows.


\subsubsection{Building the Rings}\label{top:algorithm}

The ATLAS calorimeters comprise rectangular cells in the
\etaphi plane for different layers in depth\footnote{Except for the forward calorimeters.}.
As the algorithm was considered to operate online, approximations were made. Specifically, the rectangular rings are constructed to match the grid layout of the calorimeter cells and the ring coverage is bounded by the RoI area. In addition, granularity changes from the cell sizes from different regions of the detector are neglected\footnote{The $\eta$ granularity used by the algorithm at EM1 layer is 0.003 for the entire eta range defined ($|\eta|<2.5$). However, the detector construction at this layer cover the region of $0<|\eta|<2.37$ and has different granularities in $\eta$: starts with 0.003 at $0<|\eta|<1.81$; 0.004 at $1.81<|\eta|<2.01$ and 0.006 at $2.01<|\eta|<2.37$.} in order to employ a grid-like algorithm.


Here, the algorithm RoI covers a
region of the calorimeter, centered in the L1 provided RoI, of
$0.4\times0.4$ in the $\eta\times\phi$ plane. The algorithm
starts on the second EM sampling layer (EM2)\footnote{This convention is used for all sampling layers, thus HAD1 stands for the first hadronic layer, HAD2 the second hadronic layer etc.}, where position given in $\eta\times\phi$ plane of the cell with
highest $E_T$ on the algorithm RoI is taken as the center of the cascade
interaction in the calorimeter. The initial seed cell position is propagated to other calorimeter layers in order to define the axis of the rings in that layer. Then, for each layer, the algorithm refines the seed position searching for most energetic cell inside of the window centred on the EM2 initial seed. The refined seed ($c_{hot,l}$) will be used as starting point to build the rings. A ring $R_{n,l}$ contains all cells in calorimeter layer $l$ which are $n$ cells from the refined seed (See Figure~\ref{fig:building_rings_a} for an illustration of the parameters). Formally,


\begin{equation}
%\label{eq:ring_idx}
R_{n,l} = \left\{c_{n,l} \mid n = \left\lfloor \max{\left( 
\frac{| \eta_{i,l} - \eta_{hot,l} |}{h_{\eta,l}}, 
\frac{| \phi_{i,l} - \phi_{hot,l} |}{h_{\phi,l}} 
\right)} \right\rceil, 
\forall c_{i,l} \in
\Theta_{RoI,l}
\right\},
\end{equation}



\noindent where (analogous to $\phi$ when suitable): $\eta_{i,l}$
and $\eta_{hot,l}$ are respectively the $c_{i,l}$ and $c_{hot,l}$
cells position in $\eta$; $h_{\eta,l}$ is the $l^{th}$ layer cell size in $\eta$; $\Theta_{RoI,l}$ is the set of cells
in the $l^{th}$ layer which are within the Ringer RoI and $\lfloor \cdot \rceil$ is the \textit{round} function.
\tablename~\ref{tab:ring_alg_parameters} shows the number of rings computed at each layer and the respective granularity.

\input{sections/03_ringer/tables/granularity}



If no cell is available for a given ring, the corresponding ring quantity is set to 0.
The sum of the transverse energies of cells $c_{n,l}$ in the ring $R_{n,l}$ form a vector of discriminating quantities. 
They are ordered outwards and from the
innermost layer of the calorimeter and provide the ring-shaped information 
of the RoI. Figure~\ref{fig:building_rings_b} shows the ring-shaped mean profile differences between electrons and jet particles. The implementation of this algorithm is simple and the actual code
was optimized to be fast (see Section~\ref{ssec:cpu_reduction}). 









%\input{sections/03_ringer/algorithms/ringer_algorithm}


\begin{figure}[!ht]
  \begin{center}
  \begin{subfigure}[c]{.48\textwidth}
  \centering
  \includegraphics[width=\textwidth]{sections/03_ringer/figures/reco_steps/ring_em1_mask.pdf}
  \caption{}
  \label{fig:building_rings_a}
  \end{subfigure}\\
  \centering
  
  \begin{subfigure}[c]{.7\textwidth}
  \centering
  \includegraphics[width=1.1\textwidth]{sections/03_ringer/figures/reco_steps/data17_zee_mean_rings_profiles_et2_eta0.pdf}
  \caption{}
  \label{fig:building_rings_b}
  \end{subfigure}
  \caption{
  Top: Illustration of the \fastcalo algorithm for building ring sums in the EM1, sketching of the computation of $n$ for the first four rings in a hypothetical layer.
  	Bottom: Average ring profile from collision data at $30 \leq E_T < 40$ GeV and $0.0 \leq \eta < 0.8$ for each layer of the calorimeter. The embedded profile shows the EM1 layer ring energies in a logarithmic scale. Here, it is possible to observe that most part of the energy for the electrons is absorbed by the eletromagnetic layers. For the jets, the opposite is observed and most of the energy is absorbed by the hadronic layers.}
  \end{center}
\end{figure}





\subsection{Ring Sums as Discriminant Variables}\label{ssec:ringer_id}

% TODO Add references of sucesfull aplications of NNs in other fields.


%The ring sums compose a space with distinct nature with respect to the standard
%shower shape variables, where the first contains higher redundancy between its
%variables. Although the use of univariate cuts or a likelihood model could be
%considered, they have limitations to explore concisely the statistical
%dependency.
To deal with the high redundancy between the ring variables, the benefit from machine learning processing was explored to unveil the latent discriminating space, with the specific case of Multi-layer Perceptron (MLP)~\cite{haykin_2008} being employed for Run 2 operation

%MLPs require a normalization preprocessing step~\cite{haykin_2008} to adjust the input dynamic range to the neuron activation function employed (Section~\ref{top:pp}). 
An ensemble of MLPs was employed for operation by tuning and selecting
the models for selected regions of the \eteta space (Section~\ref{top:nn_ensemble}), whose
motivations are driven by physics, instrumentation and software 
engineering. The \rnn
%engineering. In the same way, \rnn 
decision making is also based on division of the phase space in regions, and the discriminant requirement is
computed as a linear function of \avgmu to pursue signal efficiency invariance
to pile-up contributions. 
The strategy to structure the MLPs,  and train the hyper-parameters of the ensemble and tune of discriminant requirements 
%The strategy, structure of the MLPs and hyper-parameters for training the ensemble and tuning of discriminant requirements 
are described in Section~\ref{sec:tuning}.

%\begin{figure}[h!t]
%\centering
%\includegraphics[width=0.9\textwidth]{sections/03_ringer/figures/Ensemble.pdf}
%\caption{\label{fig:ensemble}
%Processing flow diagram of the \rnn{} ensemble operation and decision making
%during Run~2.
%}
%\end{figure}

%\input{sections/03_ringer/algorithms/ensemble_algorithm}

\subsubsection{Ring Sum Normalization}\label{top:pp}

The current strategy concatenates all rings in a single vector of 100
variables. An absolute normalization using the total RoI energy, as in
(\ref{eq:ring_norm}), is applied to normalize the variables. This procedure was
initially proposed and examined by~\cite{1995_seixas_ringer}, as a way of
preserving the shower energy profile in fractions of the total energy. The
absolute value is used to avoid reflecting the values along the axis due to
negative noise accumulation, a behavior which would impact the physics
representation of the normalized values and require a more complex decision
boundary:

\begin{equation}
  r^\prime_{k} = \frac{r_{k}}{| \sum\limits_{i=1}^{100} r_i
  |}, \;\;\;
    \forall \; k\in\{1,\dots,100\}.
\label{eq:ring_norm}
\end{equation}

A study on simulation data showed that this strategy had compatible
efficiency to other possible normalization schemes. It is specially valuable for its
simplicity as it uses of a non parametric approach, and for allowing easy interpretation of the shower profile. 


\subsection{Motivation for an Ensemble of Neural Networks}\label{top:nn_ensemble}

The normalized concatenated ring vector feeds an ensemble of MLPs. A single
model is drawn from the ensemble for operation based on the nearest region on the \eteta space for which it was designed.

In regards to calorimetry, the usage of an ensemble with specific models per
phase space region allows to mitigate fluctuations in the variable profiles
%(Figure~\ref{fig:ring_distortion}) 
due to the detector response and energy differences of the showers.
%response (Section~\ref{ssec:calo}). %and limitations of the online algorithm
%(Section~\ref{top:algorithm}) 
%in the observed signatures of the physics objects.
A large contribution comes from granularity changes, which are discrete
in the phase space. Other important factors 
influencing such profiles are the
amount of the material as a function of \abseta{} and the dependence of
underlying processes in the shower development with respect to the incoming
particle energy. Although these alterations are mostly continuous, it is also
possible to approach the problem using space discretization.

%Regions overlapping two distinct granularity are not handled by
%this approach. Other sources, as
%the variation in the shower profile due to the energy and amount of material
%(see Figures~\ref{fig:cal_em_x0} and~\ref{fig:cal_had_lambda}), contribute with
%modifications in the variable profiles that may not be completely mitigated with
%hard-bounded regions. Similarly, One additional contribution is
%the pseudorapidity and transverse energy measurements, which are subject to the
%uncertainties of the \fastcalo{} reconstruction.
%as the different shower uniformity~\cite{Wigmans2017}\hspace{0.01\textwidth}

\begin{comment}
\begin{figure}[h!t]
\centering
\begin{subfigure}[c]{.6\textwidth}
\centering
\includegraphics[width=\textwidth]{sections/03_ringer/figures/L2Calo_ring_9_eta0_etComp.pdf}
\centering
\caption{}%
\end{subfigure} \\
\begin{subfigure}[c]{.6\textwidth}
\centering
\includegraphics[width=\textwidth]{sections/03_ringer/figures/L2Calo_ring_9_et3_etaComp.pdf}
\caption{}%
\end{subfigure} 

\caption{Marginal distributions of the non-normalized first ring (hottest cell)
in the EM1 for the \et (a) and  \abseta (b) in Z$\rightarrow$ee simulated data
for the boundaries employed for extracting the \rnn{} discriminants in 2017.}%
%\label{fig:ring_distortion}
\end{figure}
\end{comment} 

In physics, such ensemble approach is consistent with the unfolding
strategy~\cite{unfolding_book}, usually employed for correcting for
measurement distortions due to the limitations of the instrumentation. It relies
on the definition of regions in variable space leading to these
variations where the detector efficiency, as well as other
sources of interference, are approximately constant. Once classification
efficiencies are evaluated using regions, it is natural that model parameters
are also defined for these regions. It is the case of the electron likelihood
algorithm~\cite{atlas_electron_id_offline}, which motivated the usage of
\eteta{} regions in the \rnn{} algorithm.



At the hardware level, splitting the training into multiple small models to accommodate inhomogeneity of the detector response speeds up the training cycle and allows to handle data in memory, since only a small portion of the data is used to train that specific model. Limiting the memory requirement for tuning
the models is particularly interesting in order to benefit from low-memory 
 processing nodes, which were predominantly available in 
WLCG~\cite{2015_lcg_tdr} during our
developments.\@ In addition, the decomposition of the problem can also lead to a
reduction of the training time~\cite{Polikar2006}, as observed heuristically\footnote{
  When using NVIDIA RTX 2080Ti in tensorflow 1.14 and a minibatch size of 1024, a 
  MLP with single hidden layer, a substantial increase in the runtime (from 4 
  to 41 seconds per epoch) with respect to the ensemble version was observed.} when
comparing the tuning time of the ensemble to a single and more complex model. This effect can be related to the reduction of conflicting training information, which can occur when a network is forced to learn several dissimilar functions
as distinct decision boundaries for each region~\cite{Auda1999,haykin_2008}.
Regarding the operation, the proposed ensemble only requires the choice of a
single model to operate instead of a more often committee machine approaches
that require the computation of different models in parallel with a fusion
method~\cite{zhou_ensemble}.  Therefore, the chosen ensemble configuration adds
minimal overhead in terms of trigger latency. Similarly, the choice was to
operate with low-complexity models, a single hidden layer with few neurons, to obtain a competitive method with respect to the cut-based
approach, in terms of processing speed.



\subsection{Training and Decision Making}%
\label{sec:tuning}

In this section, the details of the training and decision making tuning procedures
of the \rnn ensemble are presented. Firstly, the description of figures of merit (Section~\ref{ssec:fom}) that were employed.

The 2017 procedure (Section~\ref{ssec:2017}) derived the MLP ensemble
models with simulated data and defined the discriminant cut using
2016 collision data, targeting a signal efficiency similar to that of the cut-based trigger.
The training procedure derived one ensemble of MLP models for each one of the four\footnote{The working points are defined as: tight, medium, loose and very-loose.} working points. 

On the other hand, the 2018 training procedure (Section~\ref{ssec:2018})
employed a single ensemble structure for all working points and used
collision data for training, as was the case for the derivation of offline
and final \hlt likelihood models~\cite{aaboud2019electron}.
% (since 2017~\cite{DAQ-2018-182})





\subsubsection{Figures of Merit}\label{ssec:fom}



The sets $\Theta_{\text{e}}$ and $\Theta_{\text{b}}$ of real and fake electrons, respectively, were selected by using the \tnp{} method and the offline selection for trigger developments (Section~\ref{ssec:dataset}). Assume the existence of a model $\mathcal{C}$ representing a function $f_{\mathcal{C}} : \Theta \rightarrow \mathbb{R}$ optimized to perform the binary classification task.   From supervised optimization, the model $\mathcal{C}$ outputs $\hat{H}:=f_{\mathcal{C}}(x)$ with target $H$ for a sample $x \in \Theta$. The operation of $\mathcal{C}$ results, respectively, in the selection of the subsets $\mathcal{O}_{\text{e}|\text{e}}$ and $\mathcal{O}_{\text{e}|\text{b}}$ from $\Theta_{\text{e}}$ and $\Theta_{\text{b}}$ as trigger candidate electrons in the \fastcalo{} processing step of the \hlt{} by defining the decision making procedure $\tau : \mathbb{R} \rightarrow \left\{\text{e},\text{b}\right\}$. As there is a single output node for each neural network in the proposed \rnn{} ensemble, $\tau$ corresponds to a mapping that is a decision threshold for selecting an electron candidate when the output node is above the given threshold.  



The convenient explicit definition of the figures of merit are detailed in~\tablename~\ref{tab:figures_of_merit}.
It is possible to tune $\mathcal{C}$ to result in a proper working point, (i.e. a targeted value of \pd{} or \pf{}) by adjusting $\tau$. All possible working points of $\mathcal{C}$ are defined by the Receiver Operation Characteristic (ROC) curve~\cite{van_trees_part1}, but keeping fixed the electron detection probabilities, or signal efficiencies, with respect to the previous baseline cut-based trigger. The \spindex{}~\cite{dos2006neural} index, the square root of the product of the geometric and arithmetic averages of the efficiencies for the signal and background categories, collapses when efficiencies on either signal or background events decrease significantly. Thus, the SP index provides a unidimensional space that allows tuning $\mathcal{C}$ for obtaining high efficiency in a balanced manner for both classes, which is given by choosing the $\spmax{}:=\max(\spindex{})$ working point.




\input{sections/03_ringer/tables/figures_of_merit}






\subsubsection{Training Procedure For 2017 Data}\label{ssec:2017}

The training procedure and decision making processes are the same for all phase
space regions and a summary of the process is provided in
\tablename~\ref{tab:2017_ringer}. For the \rnn{}, $\mathcal{C}$ is an ensemble of
MLPs and each MLP is an expert model for a single phase space
region, containing its own topology and parameters.

The model parameters have been optimized using events selected on simulation datasets
(Section~\ref{ssec:dataset}). The structure is a fully-connected single
hidden layer, which may contain from 5 to 20 hidden units, an input layer with 100 inputs, one for each ring, and an output layer with a single neuron. The activation
functions for both hidden and output neurons are the hyperbolic tangent, which has been often used in MLP designs~\cite{haykin_2008}. 

To assess the statistical fluctuations of the efficiency measurements, the stratified k-fold ($k=10$) cross-validation method~\cite{haykin_2008} was used at the price of increased computational cost by repeating training and testing procedures on different
randomly chosen subsets. The stratified k-fold is
among the most common cross-validation techniques and consists of partitioning
the dataset in k disjoint subsets, each model tested with the $i$th subset and
trained with the remaining ones. The dataset stratification follows the
empirical order of the event selection, in order to allow straightforward job
parallelization, i.e.\ random permutation is not employed. The possible effect
of a temporal structure in data was neglected as the 2017 training procedure
employed simulation data.

In case of similar cross-validation efficiencies when comparing standard
error confidence interval corresponding to \SI{68}{\%} for different
configurations, the choice is based on
parsimony~\cite{haykin_2008, medeiros2001statistical} so that the structure with lower number of hidden units
is employed as higher generalization power can be expected. 



\input{sections/03_ringer/tables/2017_ringer}


For each cross-validation sort and working point, 100 networks, initiated
as from~\cite{initnw}, are optimized by back-propagating the Mean Squared Error (MSE) through the RPROP
algorithm~\cite{rprop} with all parameters set to default,
except for the parameter that regards the multiplication factor of the weight update rate when the gradient direction remains the same as of the previous iteration. It was 
%except for
%$\eta^+=1.1$ (default is 1.2). This particular
%parameter regards the multiplication factor of the weight update rate when
%gradient direction remains the same as of the previous iteration. It was
observed during Run~1 that this modification improved convergence speed. RPROP
is an adaptive gradient descendent technique that is resilient to the size of
the derivatives. This repeated model initialization aims at
avoiding poor sub-optimal solutions due to the usage of gradient-based algorithms.
%as RPROP in optimisation problems involving non-convex and complex function.
For each training procedure, only three models out of the 100 are retained: two
for retrieving the best efficiency when fixing the \pd{} or \pf{} to the
baseline \fastcalo{} efficiency and another resulting in the \spmax{} value.
Early stopping~\cite{haykin_2008} is employed to avoid model over-training. The
selection of the optimal iteration and operating model is based on an heuristic approach (multi-stop)~\cite{Goodfellow2016}.


Computational resources from the WLCG and \emph{Lobo Carneiro}
super-cluster~\cite{lobo_carneiro} were used to tune 1.3~M shallow-learning
neural networks, which resulted in an ensemble of \SI{20}{MLPs} (five in \et{}
$\times$ four in \abseta{}) for each one of the four working points used by
electron chains in the HLT (see Section~\ref{ssec:egamma_trigger}).
Here, the first training strategy considered only four regions in $|\eta|$ ($0.0\rightarrow 0.8\rightarrow1.37\rightarrow2.5$).
Except for few exceptions, the models in the ensemble employed 5 neurons in the 
hidden layer.  Thus, for 2017 operation, 
\rnn{} operation did not result in
successive contained sets for more stringent working points, i.e. an electron
candidate accepted by \medium{} is not necessarily also accepted by \loose{}
working point. %To the best of our knowledge, this does not impact the \hlt{}
%given that triggers are employed in parallel both for operation and analysis.
%Thus, eventual disagreements between their selections are irrelevant.


After the training stage, the decision is taken by applying a threshold in the
one-dimensional output node of each expert MLP.\@ 
Inspired by the HLT likelihood algorithm, the threshold applied for
\rnn is also computed as a linear function of \avgmu{}.
However, when employing the standard NN parametrisation, a non-linear
behavior was observed near the asymptote of the output activation 
function (See Figure~\ref{fig:nn_correction_with_tansig}), which occured
for the medium and tight operation points. Heuristically, it was 
observed that this behaviour can be avoided when replacing the output
activation by a linear function (Figure~\ref{fig:nn_correction_without_tansig})
for operation, after the training stage is completed.
This strategy provides a linear
behavior of the targeted signal efficiency with respect to the online pile-up
estimator. Except for the end-cap region during 2017, the boundaries for
defining discriminant requirement parameters were the same as those employed in
the \rnn ensemble (Section~\ref{top:nn_ensemble}). The 
interpolation for the \et{} values, which was employed in the likelihood as a smoothening strategy~\cite{aaboud2019electron}, was
%interpolation in
%\et{} employed in the likelihood as a smoothening
%strategy~\cite{aaboud2019electron} was 
not used (Section~\ref{ssec:egamma_trigger}).
%Tables~\ref{tab:comp_etabins}
%and~\ref{tab:comp_etbins} show the boundaries in the \eteta{} axis used for
%Run~2 operation.



\begin{figure}[h!tb]
  \begin{center}
  %\hspace{0.01\textwidth}
  \begin{subfigure}[c]{.48\textwidth}
  \centering
  \includegraphics[width=\textwidth]{sections/03_ringer/figures/th2_signal_tight_cutbased_et3_eta1_with_tansig.pdf}
  \caption{Neural output with hyperbolic tangent w.r.t pile-up.}
  \label{fig:nn_correction_with_tansig}
  \end{subfigure}
  \hfill
  \begin{subfigure}[c]{.48\textwidth}
  \centering
  \includegraphics[width=\textwidth]{sections/03_ringer/figures/th2_signal_tight_cutbased_et3_eta1_without_tansig.pdf}
  \caption{Neural output with linear activation w.r.t pile-up.}
  \label{fig:nn_correction_without_tansig}
  \end{subfigure}
  %\hfill
  \caption{
    \rnn output as a function of \avgmu{} for $Z\rightarrow ee$ probes in 
    2016 collision data.
    The computed threshold per \avgmu{} unit for achieving the target 
    efficiency is shown in blue full circle. The black line is a linear fit of the thresholds.
    The output space is computed using the trained model without modifications 
    in (a), while in (b) the output activation is replaced by the identity.
    The electron probes are selected using the training criteria.
  }%
  \end{center}
  \end{figure}




%\input{sections/03_ringer/tables/comp_etabins}
%\input{sections/03_ringer/tables/comp_etbins}



% TODO A plot showing the particular impact in the 2.37 region
% TODO Cross-validation: error bars
% TODO MLP training example
\begin{figure}[h!t]
\centering
\includegraphics[width=0.6\textwidth]{sections/03_ringer/figures/mu_2015_2018.pdf}
\caption{\label{fig:mu_2015_2018}
Luminosity-weighted distribution of the mean number of interactions per crossing
for the Run~2 pp collision data at 13 TeV centre-of-mass
energy.~\cite{atlas_lumi_run2_results}}
\end{figure}


The linear correction applied during 2017 is upper-bounded by $\avgmu=40$
collisions, a quantile encompassing a large fraction of 2016 data
(Figure~\ref{fig:mu_2015_2018}). The objective was to avoid extrapolation to a
regime not previously evaluated, which could lead to unexpected growth in the
background rate and, thus, to data-taking issues due to the large CPU demand
from the electron triggers.

% (decision taking)
To account for Monte Carlo data mis-modeling, the parameters of the linear threshold
correction were derived using 2016 collision data. Furthermore, it was observed
that the rings in the region $2.37<\abseta<2.47$ had particular profiles due to
the lack of strip cells in this region and demanded a specific discriminant requirement in order to keep signal efficiencies near triggers without the \rnn{}. In order to avoid retraining all models, it was decided to split the last $|\eta|$ region in two, from $1.37\rightarrow 2.5$ to $1.37\rightarrow2.37\rightarrow2.5$, and duplicate all models, for this region, to the new bin. Finally, the ensemble used to operates in 2017 has a total of 25 MLP models. The ensemble boundaries are defined by Table~\ref{tab:ensemble_regions}.

\input{sections/03_ringer/tables/ensemble}


%It drastically affected the ring profiles in this region which resulted in a shift
%of the signal discriminant towards the direction of the background target and
%demanded a specific discriminant requirement in order to keep signal
%efficiencies near triggers without \rnn{}.

\FloatBarrier
\subsubsection{Training Procedure For 2018 Data}\label{ssec:2018}

Most of the procedure was kept unchanged for 2018, with modifications shown in
Table~\ref{tab:2018_ringer}.
In opposition to 2017, the developments for 2018
could benefit from collision data 
representing similar data taking conditions.
%In 2018, the developments could benefit from collision data representing similar conditions to data taking. 
The major alteration in 2018 tunes was the derivation of the \rnn{} ensemble with collision data based on \Zee{} \tnp{} event selection, in order to avoid the Monte Carlo data mis-modeling.
To simplify the training strategy, the selection of three models for each training was abandoned, keeping only the \spmax{} model, as the previous approach was more complex without a clear return on trigger efficiency. 

Likewise, MLPs with 5 to 10 units in the hidden layer were tuned, as most models did not require
more than 10 units in 2017. With a larger time span for entering operation, we
added MLPs specialized in the region where a change was detected in the ring
profiles before 2017 operation between $2.37<\abseta<2.47$, resulting in the operation of
\SI{25}{MLPs}. The correction limit was set to
$\avgmu{}=100$~collisions or, in other words, removed for 2018 operation, given
that the data-taking 
conditions never reached such a large pile-up level. By
%conditions did not extrapolate this value. By 
assuming a linear efficiency extrapolation, 
it was observed that most phase space regions did not reach background efficiencies
larger than \SI{15}{\%} before arriving at $\avgmu=100$~collisions, while same
signal efficiencies were maintained.

% NOTE TriggerEgammaMeeting_20180213
\input{sections/03_ringer/tables/2018_ringer}




